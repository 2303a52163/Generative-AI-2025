{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a52163/Generative-AI-2025/blob/main/2303A52163_WEEK_3_ASSIGNMENT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION-1"
      ],
      "metadata": {
        "id": "yaJ3hKpWtz2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "64c3e860-6a2a-4836-a6f7-474f38b6ad23",
        "id": "FZr5Q79esIfG"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: x = -1.60, f(x) = 41.45, f'(x) = 26.00\n",
            "Iteration 1: x = 7.55, f(x) = 16435.74, f'(x) = -91.52\n",
            "Iteration 2: x = -858.40, f(x) = 2714760639123.23, f'(x) = 8659.53\n",
            "Iteration 3: x = 1265029831.91, f(x) = 12804808061401773275155422889883205632.00, f'(x) = -12650306903.12\n",
            "Iteration 4: x = -4048855683368040987719892992.00, f(x) = 1343690333543425041487982330856739233413262998636518806844409597051268287773246050586698066254871122429589061632.00, f'(x) = 40488556833680405479152418816.00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "(34, 'Numerical result out of range')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8b1fb986ce72>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmax_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-8b1fb986ce72>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(x0, learning_rate, epsilon, max_iterations)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {iteration}: x = {x:.2f}, f(x) = {f(x):.2f}, f'(x) = {gradient:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8b1fb986ce72>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: (34, 'Numerical result out of range')"
          ]
        }
      ],
      "source": [
        "def f(x):\n",
        "    return 5 * x**4 + 3 * x**2 + 1\n",
        "\n",
        "def f_prime(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "def gradient_descent(x0, learning_rate, epsilon, max_iterations):\n",
        "    x = x0\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iterations:\n",
        "        gradient = f_prime(x)\n",
        "\n",
        "        if abs(gradient) < epsilon:\n",
        "            break\n",
        "\n",
        "        x = x - learning_rate * gradient\n",
        "\n",
        "        print(f\"Iteration {iteration}: x = {x:.2f}, f(x) = {f(x):.2f}, f'(x) = {gradient:.2f}\")\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    print(f\"\\nMinimum value found at x = {x:.2f}, f(x) = {f(x):.2f} after {iteration} iterations.\")\n",
        "    return x\n",
        "\n",
        "\n",
        "x0 = 1.0\n",
        "learning_rate = 0.1\n",
        "epsilon = 0.001\n",
        "max_iterations = 100\n",
        "\n",
        "gradient_descent(x0, learning_rate, epsilon, max_iterations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION-2"
      ],
      "metadata": {
        "id": "XcMAKHmfuOTz"
      }
    },
    {
      "source": [
        "def g(x, y):\n",
        "    return 3 * x**2 + 5 * (2.71828**(-y)) + 10\n",
        "\n",
        "def g_x(x, y):\n",
        "    return 6 * x\n",
        "\n",
        "def g_y(x, y):\n",
        "    return -5 * (2.71828**(-y))\n",
        "\n",
        "def gradient_descent(x0, y0, learning_rate, epsilon, max_iterations):\n",
        "    x, y = x0, y0\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iterations:\n",
        "        # Calculate both grad_x and grad_y inside the loop\n",
        "        grad_x = g_x(x, y)\n",
        "        grad_y = g_y(x, y)\n",
        "\n",
        "        if abs(grad_x) < epsilon and abs(grad_y) < epsilon:\n",
        "            break\n",
        "\n",
        "        x = x - learning_rate * grad_x\n",
        "        y = y - learning_rate * grad_y\n",
        "\n",
        "        print(f\"Iteration {iteration}: x = {x:.2f}, y = {y:.2f}, g(x, y) = {g(x, y):.2f}, grad_x = {grad_x:.2f}, grad_y = {grad_y:.2f}\")\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    print(f\"\\nMinimum value found at x = {x:.2f}, y = {y:.2f}, g(x, y) = {g(x, y):.2f} after {iteration} iterations.\")\n",
        "    return x, y\n",
        "\n",
        "x0 = 1.0\n",
        "y0 = 1.0\n",
        "learning_rate = 0.1\n",
        "epsilon = 0.001\n",
        "max_iterations = 10\n",
        "\n",
        "gradient_descent(x0, y0, learning_rate, epsilon, max_iterations)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xddVE-OAvYHU",
        "outputId": "4d4b74da-068e-4e46-ad1b-89297ab7ffab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: x = 0.40, y = 1.18, g(x, y) = 12.01, grad_x = 6.00, grad_y = -1.84\n",
            "Iteration 1: x = 0.16, y = 1.34, g(x, y) = 11.39, grad_x = 2.40, grad_y = -1.53\n",
            "Iteration 2: x = 0.06, y = 1.47, g(x, y) = 11.16, grad_x = 0.96, grad_y = -1.31\n",
            "Iteration 3: x = 0.03, y = 1.58, g(x, y) = 11.03, grad_x = 0.38, grad_y = -1.15\n",
            "Iteration 4: x = 0.01, y = 1.69, g(x, y) = 10.93, grad_x = 0.15, grad_y = -1.03\n",
            "Iteration 5: x = 0.00, y = 1.78, g(x, y) = 10.84, grad_x = 0.06, grad_y = -0.93\n",
            "Iteration 6: x = 0.00, y = 1.86, g(x, y) = 10.78, grad_x = 0.02, grad_y = -0.84\n",
            "Iteration 7: x = 0.00, y = 1.94, g(x, y) = 10.72, grad_x = 0.01, grad_y = -0.78\n",
            "Iteration 8: x = 0.00, y = 2.01, g(x, y) = 10.67, grad_x = 0.00, grad_y = -0.72\n",
            "Iteration 9: x = 0.00, y = 2.08, g(x, y) = 10.63, grad_x = 0.00, grad_y = -0.67\n",
            "\n",
            "Minimum value found at x = 0.00, y = 2.08, g(x, y) = 10.63 after 10 iterations.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0001048575999999998, 2.079353864453758)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION-3"
      ],
      "metadata": {
        "id": "scFLSQijvmuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def z(x):\n",
        "    return 1 / (1 + 2.71828**(-x))\n",
        "\n",
        "def z_prime(x):\n",
        "    return (2.71828**(-x)) / (1 + 2.71828**(-x))**2\n",
        "\n",
        "def gradient_descent(x0, learning_rate, epsilon, max_iterations):\n",
        "    x = x0\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iterations:\n",
        "        gradient = z_prime(x)\n",
        "\n",
        "        if abs(gradient) < epsilon:\n",
        "            break\n",
        "\n",
        "        x = x - learning_rate * gradient\n",
        "\n",
        "        print(f\"Iteration {iteration}: x = {x:.2f}, z(x) = {z(x):.2f}, gradient = {gradient:.2f}\")\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    print(f\"\\nMinimum value found at x = {x:.2f}, z(x) = {z(x):.2f} after {iteration} iterations.\")\n",
        "    return x\n",
        "\n",
        "\n",
        "x0 = 0.0\n",
        "learning_rate = 0.1\n",
        "epsilon = 0.001\n",
        "max_iterations = 10\n",
        "\n",
        "gradient_descent(x0, learning_rate, epsilon, max_iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t6gw3VavpRt",
        "outputId": "92085d4e-0a26-4cd4-839e-86f975b79041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: x = -0.03, z(x) = 0.49, gradient = 0.25\n",
            "Iteration 1: x = -0.05, z(x) = 0.49, gradient = 0.25\n",
            "Iteration 2: x = -0.07, z(x) = 0.48, gradient = 0.25\n",
            "Iteration 3: x = -0.10, z(x) = 0.48, gradient = 0.25\n",
            "Iteration 4: x = -0.12, z(x) = 0.47, gradient = 0.25\n",
            "Iteration 5: x = -0.15, z(x) = 0.46, gradient = 0.25\n",
            "Iteration 6: x = -0.17, z(x) = 0.46, gradient = 0.25\n",
            "Iteration 7: x = -0.20, z(x) = 0.45, gradient = 0.25\n",
            "Iteration 8: x = -0.22, z(x) = 0.44, gradient = 0.25\n",
            "Iteration 9: x = -0.25, z(x) = 0.44, gradient = 0.25\n",
            "\n",
            "Minimum value found at x = -0.25, z(x) = 0.44 after 10 iterations.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.24889788013523362"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUESTION-4"
      ],
      "metadata": {
        "id": "RGaZDuOXwk9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_parameters(input_value, expected_output):\n",
        "\n",
        "    min_error = float('inf')\n",
        "    best_m = 0\n",
        "    best_c = 0\n",
        "\n",
        "    for m in range(-10, 11):\n",
        "        for c in range(-10, 11):\n",
        "\n",
        "            predicted_output = m * input_value + c\n",
        "\n",
        "            error = (expected_output - predicted_output) ** 2\n",
        "\n",
        "            if error < min_error:\n",
        "                min_error = error\n",
        "                best_m = m\n",
        "                best_c = c\n",
        "\n",
        "    return best_m, best_c\n",
        "\n",
        "input_value = 2\n",
        "expected_output = 0.5\n",
        "\n",
        "optimal_m, optimal_c = find_optimal_parameters(input_value, expected_output)\n",
        "\n",
        "print(\"Optimal M:\", optimal_m)\n",
        "print(\"Optimal C:\", optimal_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsL_F1IawmuV",
        "outputId": "e9cc4359-6319-4a8f-b8aa-c15b4c4c2707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal M: -5\n",
            "Optimal C: 10\n"
          ]
        }
      ]
    }
  ]
}