{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a52163/Generative-AI-2025/blob/main/2303A52163_WEEK_11_ASSIGNMENT_11_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "\n",
        "# Define paths\n",
        "train_zip_path = \"/content/train-20250404T145129Z-001.zip\"\n",
        "val_zip_path = \"/content/validation-20250404T145130Z-001.zip\"\n",
        "train_dir = \"\"\n",
        "val_dir = \"/mnt/data/validation\"\n",
        "\n",
        "# Extract zip files\n",
        "with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(train_dir)\n",
        "\n",
        "with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(val_dir)\n",
        "\n",
        "# Image and training parameters\n",
        "img_height, img_width = 64, 64\n",
        "batch_size = 32\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(train_data.num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adadelta(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, epochs=10, validation_data=val_data)\n",
        "\n",
        "# Evaluate\n",
        "train_loss, train_acc = model.evaluate(train_data)\n",
        "val_loss, val_acc = model.evaluate(val_data)\n",
        "\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Validation Accuracy:\", val_acc)\n"
      ],
      "metadata": {
        "id": "Xpz1E1TZNRuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "\n",
        "# Image parameters\n",
        "img_height, img_width = 64, 64  # You can adjust based on actual image sizes\n",
        "batch_size = 32\n",
        "\n",
        "# Define paths\n",
        "train_zip_path = \"/content/train-20250404T145129Z-001.zip\"\n",
        "val_zip_path = \"/content/validation-20250404T145130Z-001.zip\"\n",
        "train_dir = \"train\"  # Directory to extract training data\n",
        "val_dir = \"validation\"  # Directory to extract validation data\n",
        "\n",
        "# Extract zip files to respective directories\n",
        "with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(train_dir)\n",
        "\n",
        "with zipfile.ZipFile(val_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(val_dir)\n",
        "\n",
        "# Setup data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,  # Now using the extracted directory\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    val_dir,  # Now using the extracted directory\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# ... (rest of your model building and training code remains the same)\n",
        "\n",
        "# Build CNN model (as per Tabela 1)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(256, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(train_data.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=Adadelta(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(train_data, epochs=10, validation_data=val_data)\n",
        "\n",
        "# Evaluate accuracy\n",
        "train_loss, train_acc = model.evaluate(train_data)\n",
        "val_loss, val_acc = model.evaluate(val_data)\n",
        "print(\"Training Accuracy:\", train_acc)\n",
        "print(\"Validation Accuracy:\", val_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYQOQ-pnOFJk",
        "outputId": "6288fecf-db4d-46a1-c48b-b6a07b47f48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 images belonging to 1 classes.\n",
            "Found 4 images belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 352ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "Training Accuracy: 1.0\n",
            "Validation Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}